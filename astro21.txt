Sarah Ballard (PI) Quadry Chance (FI) 
Institution:  University Of Florida, Gainsville  
20-ASTRO20-0030, Probing the Role of Binarity in Exoplanetary System Architecture with Kepler and GAIA 
 
Large-scale exoplanet surveys now indicate that there exists no single universal blueprint for a planetary system. Rather, a wide diversity of system architectures exist. The underlying mechanisms that shape this diversity of outcomes are only partially understood. We propose to constrain the effect of binarity on planetary systems in a new way. There is a gap in our understanding in the point at which a binary companion affects planet architecture, but not prevent or destroy it altogether. Large adaptive optics surveys of individual known planet systems have increased our understanding of binarity and planet occurrence (e.g. Kraus et al. 2016), but are (1) limited to known planet systems and (2) expensive to undertake. We propose to leverage existing data from Kepler and GAIA to assign a statistical likelihood of binarity to each of the Kepler Input Catalog stars: both with and without planets. From the Kepler mission, we have planet yields in hand; to construct the binary likelihood we will leverage the (1) Kepler catalog with (2) known companions from AO surveys of Kepler stars, (2) indicators for binarity from the Gaia DR2 catalog. For each individual star, folding all of this information together, we can perform a comparison of the Bayesian evidence between a model with a single star and a model with a binary. One of the Planetary Science Division’s goals are to understand the formation and early evolution of planetary systems with its Research and Analysis program. To this end, we will provide an assessment not only of raw planet occurrence among binaries versus single stars but also an assessment of the types of planet architectures among binaries versus non-binaries, via planet transit multiplicity.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Simeon Bird (PI) Ming-Feng Ho (FI) 
Institution:  University Of California, Riverside  
20-ASTRO20-0022, Parametric Inference using Cosmological Simulations for the WFIRST Era 
 
Next generation telescopes and cosmological surveys, such as WFIRST (Nancy Grace Roman 
Space Telescope), will provide unprecedented information about the large scale structure of the Universe down to small scales. Numerical simulations must be able to make predictions for the large scale structure to a similar accuracy and scale, challenging currently available computational resources. I have developed a technique, multi-fidelity emulation, that enables accurate theoretical predictions for the matter power spectrum using only three high-fidelity simulations. As state of the art emulators require around thirty simulations, this reduces the required computational burden for numerical simulations by an order of magnitude. Our multifidelity emulation technique works by combining cheap low resolution simulations, which determine how structure depends on cosmology, with a few large and expensive simulations which determine how to model small scales. This proposal will expand the reach of the technique to cover future weak lensing and galaxy surveys. 
  
 Simulations will use the cosmological simulation code MP-Gadget developed by the PI Dr Bird, and the multi-fidelity emulator code developed by the FI Ming-Feng Ho. MP-Gadget is an NBody and hydrodynamical cosmological simulation code optimized for extreme scale. The multifidelity emulator we are developing is a statistical model which uses multi-output Gaussian processes to learn the outputs from simulations with multiple resolutions. The proposed research will apply the multi-fidelity emulator to economically learn the mapping from cosmologies to simulation outputs, by augmenting a few high-resolution simulations with many low-resolution experiments. A multi-fidelity emulator built by Gaussian processes provides a natural way to quantify the model uncertainty. Predictions may be generated from the Gaussian process quickly enough to generate sample power spectra for use inferring cosmological parameters. 
  
 Precise parameter inference plays a key role in cosmological surveys, and, currently, precision could only be achieved by using many expensive high-fidelity numerical simulations. For future WFIRST, the precision of the cosmological inference, such as measuring the density of dark matter or estimating the evolution of dark energy, will be limited by the available computing power for running expensive high-fidelity simulations. Our project will be crucial on inferring cosmologies with only a minimal cost on running high-fidelity simulations, thus enabling the inference on small-scale structure formation for the upcoming weak lensing and galaxy cluster data on highly non-linear scales. 
  
 We will use this multi-fidelity emulator to study the non-linear structure growth of the Universe by making theoretical predictions on the matter power spectrum. Our multi-fidelity emulator also allows us to study the dark matter clustering of the Universe through the gravitational weak lensing. We will also study the population statistics of dark matter haloes by making predictions on the halo mass function of the Universe. Finally, multi-fidelity modeling enables us to use Bayesian optimization to cleverly select the high-fidelity simulation to perform, allowing a faster convergence for emulator training. 
  
 The proposed work will create simulation predictions for WFIRST and is relevant to the “physics of the cosmos” program as it will help to understand the nature of dark matter and dark energy.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Philip Chang (PI) Alexandra Spaulding (FI) 
Institution:  University Of Wisconsin, Milwaukee  
20-ASTRO20-0013, FINESST: Magnetohydrodynamic Moving-mesh Simulations of Tidal Disruption Events 
 
A tidal disruption event (TDE) occurs when a star approaches within the tidal radius of a supermassive black hole (SMBH). The tidal gravity of a SMBH overcomes the self-gravity of the star, and the star is ripped apart by the tidal forces from the SMBH. Some of this tidal debris will return to the tidal radius and form an accretion disk, powering a highly luminous event.  In recent years, TDE candidates have been observed by ROSAT, Chandra and XMM-Newton.  Because of the complexity of the physics involved in these events, numerical methods are used to study aspects such as the energy distribution, fallback rate of the disrupted star, and the luminosity of the emitted jet.  However, theoretical modeling is difficult because of range of timescales along with the density of physics involved.  
  
 There are two methods used for modeling these events.  One way of modeling is using smooth particle hydrodynamics (SPH) where fluid quantities are based on a sampling of nearby particles. This method obeys conservation laws well but cannot handle discontinuities in the simulation such as shocks.  The other modeling method uses Eulerian grid-based solvers. This method is good at capturing shocks but is not as reliable in obeying conservation laws.  The solution to both these issues is to create a hybrid code that uses the strengths of both methods, which is known as the moving-mesh method.  We propose to use the moving-mesh method MANGA to analyze a tidal disruption event which has already been successful simulated in MANGA. 
  
 One important piece of physics that has been left out of many previous TDE simulations is magneto-hydrodynamics (MHD). While methods of MHD have been implemented in MM before, our proposed scheme gives an arguably greater order of accuracy.  We propose to test and validate an already implemented scheme in MANGA and apply the module to TDE studies.  In particular, we will explore the effect of magnetic field has on the disruption and explore the effects of magnetic fields on the formation of the disk results from fallback material.  
  
 Detailed theoretical understanding of these tidal disruption events is important for interpreting observations of these systems from Swift, Fermi, Chandra, Galex, TESS, and Hubble.  It is also important for future NASA missions such as WFIRST, which will likely detect many TDEs as part of its supernova survey, and JWST. These systems also may be important sources for gravitational wave observatories; future gravitational wave missions (including LISA) will benefit from this work.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Christine Chen (PI) Xinyu Lu (FI) 
Institution:  Johns Hopkins University  
20-ASTRO20-0076, Sequencing Dusty Disk Spectra: A Non-Parametric, Systematic Analysis Revealing the Relationships Between Disks and their Host Stars 
 
Debris disks provide exciting opportunities to study planet formation through their compositions and architectures. In exoplanetary debris disks, little is known about whether differentiated (crust and mantle) planetesimals are common in regions close to a star. Identification of crustal materials in debris disks is smoking-gun evidence for differentiated planetesimals, which indicates planetary evolution stages. In addition, relations between disk architectures and their host star properties reveal volatile evolution in the disk. It is unclear whether the stello-centric distance to the edge of icy belts, such as the Kuiper belt, is affected by its host star luminosity. The most comprehensive, homogeneous debris disk catalog to-date, the Spitzer IRS debris disk catalog, provides unique laboratories for studying these questions. I will systematically identify the crust-like and mantle-like minerals in debris disks in this catalog and investigate dust belt locations and grain properties as a function of stellar properties, using a non-parametric data analysis tool called Sequencer. The identification and characterization of the mineralogy of the debris disks will provide fundamental constraints on models of planet formation and evolution and will serve as a base for future studies with new, more sophisticated telescopes like JWST.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Casey DeRoo (PI) Cecilia Fasano (FI) 
Institution:  University of Iowa, Iowa City  
20-ASTRO20-0003, Quantifying the Spectral Resolution of Next-Generation Diffraction Gratings for Ultraviolet Astronomy 
 
Spectra in the ultraviolet regime (UV, 900 – 2000 Å) serve as a window into a variety of astrophysical topics by allowing us to dissect a system using measurements of the atoms, molecules, ions, and dust that radiate in UV light. Encoded in these specific colors is information about the atmospheric compositions of planets and signposts of habitability, stellar life-cycles, and energetic processes of the early universe. The next generation of UV spectrographs will require excellent sensitivity while maintaining high spectral resolution (R >30,000) to achieve their science objectives. NASA has defined a number of next-generation high-resolution spectrographs fulfilling a variety of wavelength/science needs, including Hyperion, HabEx’s UVS, and LUVOIR’s LUMOS & POLLUX and the Lynx XGS.  
  
 High resolution, high efficiency diffraction gratings are a critical component of these instruments, but the techniques needed to make to make such gratings are insufficiently mature. Motivated by advancements made in the fabrication of x-ray gratings, we will fabricate custom, flat gratings using electron beam lithography (EBL), a technique which rasters a beam of electrons to pattern photoresist. We will complete a comparative study between EBL and photolithography, a less cost- and time- intensive method which similarly writes a pattern onto photoresist using a high intensity light source. These lithography techniques will serve as masks for a subsequent KOH wet-etching process which produces atomically-smooth, blazed facets. Finally, we will examine nanoimprinting lithography (NIL) as a means of grating replication, allowing the production of many gratings suitable for flight. 
  
 We will conduct this comparative study by fabricating gratings via each method and measuring their resolution interferometrically. Interferometric measurements of the diffracted wavefronts produced by a grating allows the calculation of the period error, and hence the limiting spectral resolution, of the grating. We will determine (1) whether EBL or photolithography yields UV gratings with the highest limiting spectral resolution , (2) how blazing the grating via KOH etching impacts the spectral resolution of the grating, and (3) whether gratings replicated via NIL can preserve the performance of the master gratings. This study results in empiricallycharacterized methods for fabricating UV diffraction gratings through several promising techniques (EBL, photolithography, NIL, and KOH etching). Furthermore, we explicitly connect these fabrication techniques to UV spectrograph performance via the interferometric measurements conducted. This serves to provide essential information to future UV grating spectroscopy missions ranging from CubeSats to flagships. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Erika Hamden (PI) Aafaque Khan (FI) 
Institution:  University Of Arizona  
20-ASTRO20-0040, Understanding the Dark Current Plateau in Silicon CCDs 
 
The Hamden UV/Vis Detector lab (PI: Erika Hamden) at Steward Observatory, is at the forefront of developing single-photon counting Ultraviolet (UV) detectors for instruments to study the spectra of diffuse gas in and around the galaxies. The proposed experimental research will be conducted by Dr. Hamden’s student Aafaque R Khan for his doctoral thesis. It is divided into four related projects: 1) Dark characterization of delta-doped UV EMCCD, 2) metal-dielectric detector integrated coatings for EMCCDs, 3) Characterization of visible and delta-doped Skipper CCDs, and 4) Testing of advanced Microchannel Plate (MCP) detectors for Aspera SmallSat. The overarching goal of this work is the advancement of UV detectors for NASA-supported astrophysics missions (future flights of FIREBALL-2+, Institutional-PI: Hamden; ASPERA, PI: Vargas), future NASA-Flagship mission concepts (LUVOIR and HabEx), and proposed Explorer-class missions (e.g., Hyperion, PI: Hamden; UVSCOPE, PI: Schkolnik). 
  
 The experimental setup to conduct the detector testing and coating characterization work for these projects are available at the Hamden Lab, including a Vacuum UV (VUV) monochromator, UV detectors, vacuum pumps, detector controllers, etc. Dr. Hamden will train and advise the graduate student in conducting the proposed experiments using well-documented, standard practices in this field. The training of the graduate students to develop these setups and conduct these experiments is an important aspect of the proposed work.  
  
 The proposed work meets the primary objective of NASA FINESST solicitation of supporting research led by a graduate student, Aafaque Khan, under the supervision of the Principal Investigator, Dr. Hamden. This proposed work supports the goal of the Astrophysics Division of the NASA Science Mission Directorate (SMD) to “Explore the origin and evolution of the galaxies, stars...” by the advancement of detector technology for future missions.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Chip Kobulnicky (PI) Ashley Piccone (FI) Institution:  University Of Wyoming  
20-ASTRO20-0119, A First Measurement of the Extra-Planar Milky Way Extinction Curve 
 
Dust permeates the interstellar medium and is therefore paramount to our understanding of both Galactic and extragalactic objects. Dust extincts and reddens background light, and corrections for dust must be applied to most astronomical observations. Knowledge of the distribution of grains and the extinction properties of dust in the Milky Way are essential for accurate measurements, especially if the dust along a line-of-sight differs significantly from the Galaxy average. My project will characterize the extinction curve as a function of height above the Galactic Plane, Galactic radius, and proximity to star-forming regions. It represents the first attempt at constructing an extinction curve for the Milky Way above the Plane. I have developed and optimized the Incremental DEreddening Along Line-of-Sight (IDEALS) technique, which removes the foreground dust from any field-of-view, incrementally correcting the line-of-sight using the best fit for the reddening and extinction parameters, Rv and Av. By correcting for multiple dust populations along the line-of-sight, I can scrutinize trends in Rv with height above the Plane. Early results from this technique suggest that dust grains above the Plane may be larger than the Milky Way average.  
  
 I will accomplish this project by fusing publicly available UV through IR data from NASA and other missions. I will examine three lines-of-sight in 96 regions (totaling 288 fields of view) across the entire sky in a regular grid, mapping Rv with height above the Plane and location in the Milky Way. This will result in an extinction map that future observers can use to accurately correct for dust, as well as a published and documented Python code to determine the foreground-corrected extinction along the line-of-sight to any star. My project is crucial to NASA’s goals and will influence subfields across astronomy. An extinction curve above the Plane is essential for the accurate calibration of measurements that probe the origin and evolution of our universe. Changes in Rv throughout the Milky Way may indicate the necessity to correct for similar changes in the dust distribution of other galaxies when viewing objects within or near them. An Rv differing significantly from the Milky Way average could have implications for reconciling H0 from distance ladder measurements with Cosmic Microwave Background measurements. A spatially variant Rv could impact the follow-up of neutron star and black hole mergers. Accurate extinction measurements are also important for calculating galaxy luminosity and stellar mass functions in the local universe. My upcoming paper, Piccone and Kobulnicky (in prep), shows the proof of concept for the IDEALS technique. The above applications, and many more observations that correct for dust, show the necessity of extending it to other sightlines. I will use my science communication background to disperse my work, encouraging other collaborations to use the available map and IDEALS technique when conducting their own studies. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Michael Line (PI) Aishwarya Iyer (FI) 
Institution:  Arizona State University  
20-ASTRO20-0102, Determination of Fundamental Properties of M-Dwarf Atmospheres 
 
About 70% of stars in the Milky Way galaxy are M-dwarfs. They span a range of low masses (0.08 - 0.6 solar mass), have faint luminosities (less than 5% solar luminosity), and temperatures ranging between 2500 K - 4000 K, facilitating molecule formation throughout their atmospheres. Standard stellar analysis methods to accurately characterize them have thus far proved challenging—in not correctly accounting for vast variations in their atmospheric structures. Our proposed modeling framework will determine fundamental atmospheric properties of M-dwarfs with a two-pronged approach: a physically motivated self-consistent radiative-convective model grid with up-to-date opacities, constrained by thermo-chemical and hydrostatic equilibrium, radiative and convective transport as well as a data-driven Bayesian retrieval technique. We will leverage the broadband molecular absorption features occurring in low-resolution M-dwarf spectra (from SpeX Prism Library) and use them to our advantage in acquiring robust constraints on the effective temperature, surface gravity and bulk chemical properties of M-dwarf atmospheres. Our approach lies in the interface between the standard self-consistent prescription for modeling stellar atmospheres and an assumption-free data-driven retrieval method, where new physical intuition can be obtained about our understanding of low-mass stars.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Jessica Lu (PI) Casey Lam (FI) 
Institution:  University of California, Berkeley  
20-ASTRO20-0084, Revealing the Milky Way's hidden black hole population with microlensing 
 
Stellar-mass black holes (BHs) are an end product of stellar evolution formed when massive stars exhaust their fuel and gravitationally collapse upon themselves. The number, mass function, velocity distribution, and binary fraction of the Milky Way's BHs are the key to understanding massive stellar evolution, constraining BH dynamics and interactions, and providing context for binary-BHs detected via gravitational waves. However, all these basic properties are almost completely unconstrained, with even the total number of BHs in the Milky Way uncertain to orders of magnitude. This is due to the fact that all BHs detected in the Milky Way are in binary systems-- no isolated BHs have ever been confirmed. 
  
 One way to detect isolated BHs is through gravitational microlensing. As a massive object passes in front of a background star, it will gravitationally lens the light of the background star, producing transient photometric and astrometric signals. Photometric microlensing has been used to identify BH candidates in the past, but because of a fundamental degeneracy, it cannot be used to determine the mass of the lens. Astrometric microlensing has the power to break this degeneracy, and combined with photometric microlensing, can measure the mass of the lens object. 
  
 From previous work, I have a sample of 10 isolated BH candidates with mass, distance, and proper motion measurements obtained from photometric and astrometric microlensing data. Based on the selection criteria of this sample, this sample was expected to yield ~4 BH detections, however, there are no definitive (>5sigma) detections, and only one marginal 1sigma detection. 
  
 This proposal seeks support to attain two objectives: 
1)	Use the sample of isolated BH candidates to improve the constraint on the number of BHs in the Milky Way and make the first constraint on the isolated BH mass and velocity parameter space. To achieve this requires understanding the selection effects of the sample, which will be done with a microlensing simulation called PopSyCLE. Understanding the sensitivity of the microlensing method is also necessary, and will be done via simulating microlensing events and extracting their parameters to understand the precision and accuracy of the method. With an understanding of selection effects and the experiment’s sensitivity, I can transform this sample of 10 isolated BH candidates into constraints on the isolated BH population. This objective is relevant to the Astrophysics SMD Division Physics of the Cosmos Program’s goal to probe the nature of black holes, and the Cosmic Origins Program's goal to explore the evolution of stars that make up our universe. 
2)	Identify a strategy that will allow the Roman Space Telescope’s Galactic Exoplanet Survey 
(RGES) to detect and measure the masses of isolated BHs. To do this, I will update the PopSyCLE microlensing simulation with our new constraints to produce a more realistic BH population. I will then simulate photometric and astrometric observations of the Roman mission, and use a Baysian nested sampling analysis pipeline to determine how well the parameters can be extracted. I will do this for different RGES survey structures, to see their effect on the BH yield. This objective directly supports a NASA flagship mission in development. As the RGES design will be finalized in the next 3 years, this work is critically timed to investigate ways to maximize the BH yield and ensure the full astrometric capabilities of the mission are realized. This also aligns with NASA Science Plan (Science 2020-2024: A Vision for Scientific Excellence) Strategy 3.4 to actively encourage students to take more hands-on engagement with NASA's missions and research.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Philip Mauskopf (PI) Sasha Sypkens (FI) 
Institution:  Arizona State University  
20-ASTRO20-0123, Multiplexed readout of superconducting detector arrays using nonlinear kinetic inductance 
 
We propose to develop optimized magnetometers and current sensors that make use of nonlinear kinetic inductance in superconducting nanowires. The standard for high sensitivity cryogenic magnetic field sensors is the superconducting quantum interference device (SQUID) which is used to read out arrays of superconducting detectors such as transition edge sensors (TES) and metallic magnetic calorimeters (MMC). These detectors are proposed to be used for many of the proposed next generation NASA Astrophysics missions such as the Origins Space Telescope and the Lynx X-ray Observatory. Nanowire-based readout components could replace 
 
some of the SQUIDs and enable simpler and more robust system integration. 
  
 Nanowire-based magnetic field sensors have been demonstrated in our lab but with nonoptimized sensitivity. We propose to design optimized devices which offer up to a 400x improvement in responsivity to magnetic flux and therefore can obtain comparable sensitivity to SQUIDs. We have made prototype devices which exhibit strong magnetic field sensitivity in a moderately magnetically shielded 4 K test bed. We propose to integrate a fully shielded test facility and compare sensitivity of SQUIDs with nanowire kinetic inductance magnetometers (KIM). We then plan to integrate the nanowire magnetometers with superconducting MMCs and demonstrate a multiplexed readout.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Michael McDonald (PI) Michael Calzadilla (FI) 
Institution:  Massachusetts Institute of Technology  
20-ASTRO20-0037, Evolution of the AGN Feedback Cycle in Galaxy Clusters 
 
Over the past decade, a wealth of observational evidence from galaxy clusters in the local universe has established AGN feedback as a favorite solution to the cooling flow problem. In this scenario, the energy output of an accreting supermassive black hole is capable of preventing runaway cooling of the intracluster medium. Only recently have missions like SPT and Planck enabled the study of more distant galaxy clusters via the Sunyaev-Zel’dovich effect. We will discuss the prospects of using high-redshift clusters to study AGN feedback over cosmic time, and thus, the evolution of the cooling flow problem. A case study of the most extreme outburst observed yet at high redshift (SPT-CLJ0528, z=0.768) may hint at feedback being more bursty at higher redshifts. On the flip side, we will also look at a sample of massive, relaxed systems, such as the Phoenix cluster, whose extreme multiphase cooling could hint at a saturation point in AGN feedback. Finally, we describe our campaign that is underway to study a large sample of systems from the SPT cluster catalog in between these two extremes, up to z~2, using multiwavelength X-ray, optical, and radio data in an attempt to learn how long the AGN feedback cycle has been in place, whether it was more or less effective in the past, assess the origin of thermal instabilities, and study the assembly history of BCGs. Our one-of-a-kind sample is currently the best and only dataset capable of answering how the largest galaxies, their central supermassive black holes, and their large-scale environments have grown and co-evolved over the past ~10 Gyr, giving our research program significant alignment with the vision of NASA’s Astrophysics Division’s Physics of the Cosmos program.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Maryam Modjaz (PI) Marc Williamson (FI) 
Institution:  New York University (Inc)  
20-ASTRO20-0146, A New Spectral Modeling Framework for Re-Constructing the Explosions of Massive Stripped Stars 
 
Despite decades of research and the accumulation of hundreds of observed spectra, crucial questions regarding the progenitors of the explosions of massive stars remain unanswered. Fundamental progenitor properties like the elemental compositions are only loosely constrained, with particular uncertainty around the mass-loss (i.e. stripping) of Hydrogen and Helium from the outer layers of progenitor stars prior to explosion. The primary bottleneck in understanding the progenitors of these stripped supernovae is not a lack of data, but rather the lack of a theoretical modeling framework for interpreting spectra and making inferences about the progenitors. The FI proposes to develop such a framework using the open source spectral modeling code TARDIS and the machine learning technique of emulators. The proposed framework will enable full Bayesian fitting of theoretical models to observed stripped supernova spectra for the first time.  Furthermore, the proposed framework will be fast enough to allow self-consistent modeling of statistical samples of supernova spectra as opposed to the practice of the current paradigm where computational costs restrict researchers to producing only small numbers of models for only a handful of the most interesting objects.  The FI will use the developed framework to answer two outstanding questions for stripped supernovae: 1) How much Helium can be hidden in the ejecta of stripped supernovae and what are the consequences for progenitor mass loss?  2) Why are some supernovae also associated with gamma ray bursts, while most others are not? The proposed framework will be developed openly on GitHub and provided to the community.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Erica Nelson (PI) Justus Gibson (FI) 
Institution:  University Of Colorado, Boulder  
20-ASTRO20-0200, A Newly Resolved Window into Galaxy Growth and Quenching with Pirate 
 
When we measure the light given off from galaxies what we are observing is an amalgamation of the various physical processes that occur within. Light is being given off by populations of stars both young and old, there’s light coming off from the black hole engine at the center of the galaxy, and all of this light has to propagate through the dusty and gaseous environment of the galaxy before being free to travel to our telescopes. Much effort has been spent trying to transform this complicated mixture of photons into the underlying physical properties of galaxies such as stellar mass and star formation rate. Stellar population synthesis is the method by which the total light from galaxies at different wavelengths is transformed into physical properties by modeling it as the combination of stars, their evolution across time, dust, and other relevant quantities. This methodology has allowed us to measure key galaxy properties such as masses, star formation rate, star formation history, dust, metallicity, and ages of galaxies across the Universe. 
  
 Much has been learned, but as the quality of our data and the sophistication of our modeling techniques improve, new tools must be developed that can take advantage of these developments. There is a large variety in galaxy images with varying resolutions and pixel scales. Depending on the imaging wavelength and distance to any given galaxy you may have particularly high-resolution data if you were to observe with nearby galaxies and optical or UV wavelengths, but at longer wavelengths, resolution is much poorer. However, all of the light is important with different wavelengths being sensitive to different regions and processes. Furthermore, galaxies are often modeled based on the total amount of light in different wavelengths within some region around the galaxy, but if we want to resolve the inner structures of galaxies then we need to be modeling the individual pixels within galaxies and utilizing all of the available data to probe the cosmic origins of galaxies even deeper. 
  
 There are many outstanding questions about how galaxies form and evolve over billions of years and understanding this story is key to understanding our place in the cosmos. These questions have to do with how the stellar and structural properties of galaxies are changing and what the relationship between the two is.  To explore this, we need to be able to resolve individual regions within galaxies so that we can see where stars are forming, where stars are forming, where stars are not forming, where densities are elevated, and where dust is located. No tool exists today that can resolve these individual regions, but we propose to provide this with a new software, Pirate. Pirate will combine imaging data of varying resolution and model the individual pixels to arrive at spatially resolved maps of galaxy properties. With this tool in hand, we will finally be able to look inside galaxies and understand the details of how the structural properties of galaxies are linked to their stellar populations and what that tells us about how the origins of galaxies and how they have evolved. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Nikhil Padmanabhan (PI) Xinyi Chen (FI) 
Institution:  Yale University  
20-ASTRO20-0017, Extending cosmic density field reconstruction for upcoming large-scale structure surveys 
 
The next-generation large-scale structure surveys, such as Roman, SPHEREx, and Euclid, can reach sub-percent precision in measurements of cosmological parameters and are enormously powerful to study and potentially solve outstanding problems in cosmology. These include understanding the nature of dark energy, one of the most mysterious and fundamental questions in astrophysics, and other problems, such as the sum of the neutrino masses and inflation. A key approach to studying dark energy is measuring the baryon acoustic oscillation (BAO) distance scales in galaxy power spectra using large-scale structure surveys. In order to sharpen the BAO signature that is dampened by late-time nonlinear evolution, a method called cosmic density field reconstruction has been developed and applied in the past decade, which has reduced uncertainties in BAO scale measurements by about a factor of two. In recent years, multiple algorithms for reconstruction have been proposed. I have performed a detailed analysis of a new iterative algorithm in comparison with a standard algorithm that has been used on prior observations. I demonstrate that the iterative algorithm performs better than the standard in multiple aspects. With two promising algorithms and codes ready, I propose to extend reconstruction to probe more cosmological problems with a three-pronged approach that will help better constrain dark energy, the sum of the neutrino masses, and inflation, using NASA’s upcoming large-scale structure surveys.  
  
 First, I propose to model the full shape of the post-reconstruction power spectrum by constructing an emulator that takes as input the cosmological parameters and is applicable across a broad range of parameter space. This will allow us to constrain more cosmological parameters than have been focused on in the past, including the sum of the neutrino masses and the growth rate of the initial perturbations. Second, I propose to reconstruct higher-point functions and model the residuals that reconstruction produces under both Gaussian and non-Gaussian initial conditions. I will also estimate the parameter that characterizes the primordial non-Gaussianity from higher-point functions before and after reconstruction. A better understanding of the effects that reconstruction has on higher-point functions will help us to better probe primordial nonGaussianity and to subsequently study inflation. Third, I will augment our reconstruction framework to account for Roman’s depth variation issue in order to minimize the systematics for the application of reconstruction to Roman. This will help enable Roman’s full potential to constrain dark energy and other cosmological parameters. 
  
 This proposal is directly related to NASA’s strategic objective in the astrophysics division and a focused program: physics of the cosmos. It will also directly benefit the Roman Space Telescope through the study of the effects of depth variation specific to Roman in reconstruction. The machinery will be useful for other NASA missions such as Euclid as well. My proposed project will help Roman, Euclid and SPHEREx to more fully realize their potential to probe cosmology and study the outstanding questions in cosmology.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Massimo Ricotti (PI) Chongchong He (FI) 
Institution:  University of Maryland, College Park  
20-ASTRO20-0151, Multiscale Radiation-MHD Simulations of Compact Star Clusters and the Formation of Intermediate-Mass Black Holes 
 
Understanding star formation lies at the center of many fundamental topics in astrophysics: the nature of the first sources of radiation, the formation and evolution of galaxies, the synthesis of elements, and the formation of planets and life. Observational evidence indicates that almost every large galaxy has a supermassive black hole (SMBH) at its center. However, the origin of SMBHs remains an open question. Compact star clusters near the center of galaxies in the early universe are very promising candidates for the origin of the formation of black holes that seed the growth of SMBHs. Here we propose to conduct the first simulations of the formation of massive compact star clusters and their dynamical evolution self-consistently. The goal is to improve our understanding of the properties and dynamics of the first compact clusters, with a focus on their potential role in the production of SMBH seeds. 
  
 We approach these objectives by combining resolved simulations of the formation of compact massive star clusters with n-body simulations of their dynamical evolution. We have conducted a series of simulations of the collapsing of isolated turbulent giant molecular clouds using RAMSES-RT, a state-of-art radiation-magnetohydrodynamics code. We propose to run simulations that zoom into regions around prestellar cores and push the spatial resolution limit to as high as ~10 AU while simulating clouds of tens of parsec in size. With this approach, we are able to resolve circumstellar disk fragments that become low-mass stars. 
  
 The understanding of circumstellar disk fragmentation will improve our understanding of the stellar initial mass function and the dynamics of star clusters. We then propose to follow up and use a direct N-body code, NBODY6++, along with semi-analytic methods, to perform calculations of the dynamical evolution of the star clusters from our resolved hydrodynamic simulations. Our focus will be on the formation and growth of intermediate-mass black holes in compact star clusters. This channel of black hole formation and growth may be essential to understanding the origin of SMBHs. 
  
 We also propose to produce observational spectroscopic signatures of JWST that can be used to understand the environment of high-redshift star-forming regions. 
  
This proposal has direct relevance to many NASA Astrophysics research programs and missions. 
In the context of the SMD 2014 Science Plan, it addresses the focused program -- Cosmos Origins: exploring exploring the origin and evolution of the galaxies, stars, and planets that make up our universe. Under this scope, we will explore the formation of stars and the origin of SMBHs at the center of most galaxies. Our work will also suggest observational signatures for the James Webb Space Telescope, which is expected to launch in 2021 and to image the first light after the Big Bang. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– 
Jannick Rolland (PI) Jessica Steidle (FI) Institution:  University Of Rochester  
20-ASTRO20-0006, Improving optical design of future space-based observatories and instruments through applications of pupil aberration theory 
 
In imaging systems such as telescopes, there are two types of issues to manage that we refer to as “image aberrations” and “pupil aberrations.” Image aberrations are those that either blur or warp the image. Pupil aberrations, on the other hand, affect the quality of the image of an aperture stop in the sense that the location of the image of that stop is not well localized in space. These aberrations are critical in some system architectures. If pupil aberrations are severe, the location of the exit pupil cannot be identified. So how can one place a needed filter in a location that cannot be identified? That is one of many questions one may ask when understanding the impact of pupil aberrations. The literature is sparse on this topic, despite its importance in various imaging systems. Unlike imaging aberrations, pupil aberrations are unlikely to affect image quality directly. As importantly, however, pupil aberrations affect other critical system properties such as pupil matching, pupil masking, deviation from expected brightness, the departure from a condition known as “telecentricity” where specific rays in a bundle are supposed to be parallel, and the sensitivity of image warping to object position. More thorough knowledge of pupil aberrations can lead to insight into system design, understanding of how image aberrations change with object shift, and understanding induced aberrations, where the aberrations from one surface in a system affect the next. With this research, we plan to investigate pupil aberrations in theory and determine how they can be used to improve system design. 
 
  This project will start with an exploration of the aberration theory. We will investigate the connection between image aberrations and pupil aberrations for plane-symmetric systems and expand previous work to include calculations of higher-order aberrations from system parameters. These aberrations have become highly relevant in optical instrumentation as specifications are pushed to higher requirements. We will then apply the developed theory to system design. The exact nature of this application will be determined in consultation with partners at NASA and will include the design of two types of optical systems to answer several questions about how pupil aberration control can be effectively used in system design. Also, broadly applicable to the optics community at large, including NASA, is that the last ten years have brought a revolution in optical design with “freeform” surfaces that have enabled compact 
and high-performance imaging systems. Freeform surfaces can be defined as surfaces with no axis of rotational invariance (within or beyond the optical part). When designing with freeform optics, if desirable, we will apply best-design practices, including integrating what we learn about pupil aberrations, to design for manufacture to optimize manufacturability estimates and thus reduce costs. 
 
 In discussion with colleagues at NASA, we learned that most NASA mission instruments could benefit from significantly improved designs if pupil aberrations were considered. Given the effects of pupil aberrations, pupil aberrations can be critical for systems such as telescopes with multiple instruments as well as coronagraphs that use masking to block direct starlight to study nearby objects or find extrasolar planets. Several missions within the Astrophysics Division would benefit from an investigation into pupil aberrations. These include the Large UV/Optical/IR Surveyor (LUVOIR), the Origins Space Telescope (OST), and the Laser 
Interferometer Space Antenna (LISA). LUVOIR and Origins Space Telescope (OST), and the 
Laser Interferometer SpaceAntenna (LISA). LUVOIR and OST were part of the Astrophysics Decadal Survey, and LISA is an ESA mission with NASA support. Overall, this research will reveal the role of pupil aberrations in optical systems, with broad impact in Space Optical Instruments. We expect that this impact will influence a wide range of other applications in the future beyond this specific research. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Aomawa Shields (PI) Vidya Venkatesan (FI) Institution:  University Of California, Irvine  
20-ASTRO20-0187, Life Beyond the Outer Limits: A Comprehensive Study of the Effects of Surface Ice Composition on the Long-Term Climate and Habitability of Exoplanets Outside of the Habitable Zone 
 
The  search  for  life  beyond  the  Solar  System  focuses  on  planets  within  the  habitable  zone—the range of distances around a host star where temperatures and pressures are amenable to surface liquid water.   However,  the traditional HZ boundaries assume Earth-like, circular orbits, neglecting eccentric planets (e > 0.1), which comprise 50% all discovered exoplanets. Eccentric rocky planets may veer within and outside of the HZ throughout the course of their orbits, giving rise to temporal habitability.  At large orbital distances from their stars these planets may exhibit surface temperatures that allow for condensation of atmospheric species, leading to the formation of exotic ices—ices other than water ice —on their surfaces. Accurately simulating the climates of eccentric planets with a range of possible surface ice compositions is beyond current model prescriptions.  
  
 The scientific goal of this work is to quantify the effects of surface ice composition on the longterm climate and habitability of eccentric planets, as they have not yet been explored. To meet this goal, I will:   
  
(1)	Modify three-dimensional (3-D) global climate models (GCMs) to incorporate albedo parameterizations for the formation of a range of exotic ice compositions. 
(2)	Run climate simulations to calculate the effects of exotic ices on the climate and habitability of eccentric planets throughout their orbits.   
(3)	Make my model prescriptions and data products available to the theoretical and observational exoplanet communities via the Exoplanet Climate Database 
(exoclimates.ps.uci.edu) maintained by the Shields Center for Exoplanet Climate and Interdisciplinary Education (SCECIE) at UC Irvine. 
  
 This work will result in: (1) The most sophisticated GCM prescriptions for modeling planets subject to the formation of a range of frozen surface species; (2) a quantified assessment of the effects of orbital eccentricity and surface ice composition on the boundaries of habitability; and (3) Publicly available model parameterizations, climate, and simulated observational data to be used by the entire exoplanet community for future studies of habitability and observability of eccentric planets.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Alexander Tchekhovskoy (PI) Danat Issa (FI) 
Institution:  Northwestern University, Evanston  
20-ASTRO20-0180, Simulating neutron star merger remnant outflows and their electromagnetic signatures 
 
In August 2017, the ripples on the fabric of spacetime, known as gravitational waves, caused by two neutron stars’ violent collision 130 million light years away from us, followed by a detection of a flash of gamma-ray photons, signalled the birth of an exciting field - multi-messenger astronomy. Previously disjoint research paths in astrophysics, general relativity and nuclear physics merged together allowing us to study extreme states of matter in an unusually controlled setting.  
  
 Binary neutron star merger culminated in the formation of a black hole and a disk of hot and dense plasma. This material began plunging into the black hole, which led to the emergence of magnetized ultrarelativistic outflows, or jets, whose emission we later observed as a short gamma-ray burst (SGRB). These jets eventually pierced through the interstellar medium (ISM) sending shock waves, which gave rise to afterglow emission, which we still observe today.  
  
 However, not all plasma was destined to be consumed by the black hole; some of it escaped in the form of slower moving outflows. These outflows, initially hot, dense and opaque, as time went by, expanded, cooled down and became transparent, producing so-called “kilonova” emission. Kilonova, or the infrared/optical/ultraviolet glow, is believed to be powered by the radioactive decay of heavy elements synthesized in the aftermath of the merger, and provides us with a unique opportunity to study the r-process nucleosynthesis in an unusually controlled astrophysical setting for the first time. Information about the neutron star merger and its outcome is encoded in each of the abovementioned electromagnetic (EM) messengers. To boost the scientific return from these exciting observations, we need first-principle models to quantitatively interpret them.  
  
 I will approach this problem by carrying out the first long-duration 3-dimensional general relativistic magnetohydrodynamic (GRMHD) simulations of the merger aftermath, including (a) realistically weak seed toroidal magnetic fields, (b) accurate neutrino transport and (c) realistic equation of state, which will allow me to the predict EM emission from from first principles. In addition to this, I am planning to enhance the realism of the simulations by (1) initializing them with an outcome of fully general relativistic merger simulations, (2) including the dynamical ejecta in the picture and its effect on the jet propagation, (3) evolving the jets out to the large distances and computing the afterglow emission lightcurve, and (4) tracking the r-process nucleosynthesis in the disk ejecta to estimate the kilonova lightcurve. This is possible thanks to massively parallel GRMHD code H-AMR, which is developed by our group. Its main advantage is that it has been accelerated by using graphics processing units (GPUs), which have proven to be well-suited for simulations of the astronomical flows due to their sheer parallel processing performance.  
  
 This project directly supports the science goals of the NASA Astrophysics Division, given that it is (1) a theoretical investigation of the dynamics of the neutron star mergers and their EM emission mechanisms, and (2) development of computational methods to model these complex phenomena in the extreme environments.  
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Andrew Wetzel (PI) Isaiah Santistevan (FI) 
Institution:  University Of California, Davis  
20-ASTRO20-0077, Modeling the Cosmological Evolution of Satellite Dwarf Galaxies in 6D Phase Space 
 
By using cosmological zoom-in simulations of MW/M31-mass galaxies, I am investigating the implications of using idealized vs. cosmological simulations to derive orbital properties of the present-day satellite population. I will investigate where the two methods agree, where they diverge, and why. Understanding the limitations of using idealized simulations will give key insight into how to properly apply orbit modeling in a cosmological context. Extending my dynamical analysis to the MW and M31, I will subsequently model the orbital histories of all known satellites, and will provide the most rigorous, cosmologically informed orbit histories of all satellites in the LG. My work will make use of existing HST and Gaia data, as well as make predictions for future proper motion measurements from HST and JWST. 
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– Rosemary Wyse (PI) Carrie Filion (FI) 
Institution:  Johns Hopkins University  
20-ASTRO20-0035, The Low-Mass Stellar Initial Mass Function in the Earliest Galaxies 
 
Objective:  
 The proposed research aims to provide insight into the stellar initial mass function (IMF, the distribution of stellar masses formed in any single star formation event) of the early universe by constraining the IMF in a carefully selected sample of nearby fossils of the earliest galaxies, ultra faint dwarf galaxies (UFDs). The physics of star formation remains uncertain, and a critical aspect of how stars form is the IMF. Stars of different masses create, and eject upon their death, different elements on different timescales. Massive stars are very hot and short-lived, and strongly affect their environment in both their life and death. Low-mass stars, below a solar mass, have long lifetimes and remain unevolved for more than ten billion years. As such, the IMF has far-reaching implications for all areas of astrophysics - from the chemical evolution of galaxies to cosmic re-ionization. It is unknown if the IMF is invariant, as empirical data for a range of different populations in the Milky Way suggest, or dependent on galaxy properties, as many theories predict. It is particularly uncertain if the IMF at the low-mass end is invariant or if it depends on factors such as metallicity (the measurement of iron relative to hydrogen). The low-mass stars that formed in early galaxies, like the UFDs, are still alive today and comprise the fossil record of star formation in these galaxies. These long-lived, low-mass stars can be used to constrain the IMF, as is proposed here. The sample of UFDs selected for this study encompass a range of (low) metallicities and estimated total stellar masses, which will enable the investigation of possible trends of the IMF with galaxy properties.  
  
 Methods: 
 This study aims to constrain the low-mass IMF in five nearby UFDs to investigate if, and how, the IMF varies in these systems. In particular, possible trends of the IMF with a galaxy's mean metallicity and total stellar mass will be investigated. This study will take advantage of archival Hubble Space Telescope (HST) imaging of the galaxies in the study, and will measure the apparent brightness of each star in the imaging to produce catalogues of stellar brigthnesses and positions. The resulting catalogues will be made publicly available. A modern Markov Chain Monte Carlo modelling technique will be used to constrain the low-mass IMF from these catalogues. This technique is based on simulating stellar populations with different IMFs and comparing the simulated populations to the real population. The simulated stellar populations are generated using stellar evolution models and an assumed distance, metallicity distribution, and age for each galaxy. After many iterations of simulation and comparison the probability distribution of the IMF parameters is retrieved.  
  
  
 Relevance: 
 The proposed research directly supports the goals of the Cosmic Origins program of NASA's Astrophysics Directorate. The characterization of the IMF is crucial to understanding the origin and evolution of galaxies and their stars, one of the core objectives of the NASA Astrophysics Directorate. This work contributes to achieving the directive set forth in the Astrophysics Roadmap ""Enduring Quests, Daring Visions - NASA Astrophysics in the Next Three Decades"" of characterising the IMF of external galaxies. This work makes use of archival HST imaging, and paves the way for future studies of the low-mass IMF using the James Webb Space Telescope 
